{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "TrashClassifier.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ming-90/Kaggle_Study/blob/main/5.%20TrashClassifier/Trash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ada645-5ee8-4ad5-857e-dc90fa08970a"
      },
      "source": [
        "# [이미지/이지AI/이미지 분류] 수중 해양 쓰레기 종류 분류 모델\n",
        "\n",
        "이 노트북은 셀을 차례로 실행하여 수치 분류 과제의 전반적인 과정을 수행해볼 수 있게 제작되었습니다. 파일 경로를 추가하는 것 외에는 큰 코드 수정 없이 실습해 볼 수 있습니다."
      ],
      "id": "d7ada645-5ee8-4ad5-857e-dc90fa08970a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68e72fd0-ca13-49fc-9e92-bb9c411dacc7"
      },
      "source": [
        "## 과제 설명\n",
        "해양 침적 쓰레기 이미지로부터 이미지 내에 존재하는 해양 쓰레기 유형을 10개의 분류 클래스에 따라 분류하는 모델 개발\n",
        "\n",
        "### **데이터 설명**\n",
        "- 학습 데이터 : 해양 침적 쓰레기 이미지(.jpg) 7187 장\n",
        "- 라벨 데이터 : 이미지 파일명, 이미지 속 쓰레기 종류 정보가 표 형태로 담긴 데이터 (.csv) 1장\n",
        "\n",
        "### **입출력**\n",
        "- Input : 해양 쓰레기가 존재하는 수중 촬영 이미지\n",
        "- Output : 쓰레기 (10종) -'bundle of ropes', 'circular fish trap', 'eel fish trap', 'rope', 'fish net', 'other objects', 'rectangular fish trap', 'tire', 'spring fish trap', 'wood' 중 하나\n",
        "\n",
        "### **데이터셋 구성**\n",
        "- Train\n",
        "    - `images/` : 7187장의 jpg 파일\n",
        "    - `train.csv`  : 이미지 파일명과 쓰레기 유형 정보가 표 형태로 담긴 csv 파일 1개\n",
        "- test\n",
        "    - `images/` : 2376장의 jpg 파일\n",
        "\n",
        "\n",
        "### 베이스라인 사용 모델\n",
        "- [EfficientNet B0](https://pytorch.org/vision/master/_modules/torchvision/models/efficientnet.html)\n",
        "- 이 외에도 이미지 분류에 자주 사용되는 모델에는 CNN, ResNet, EfficientNet 등이 있습니다.\n",
        "\n",
        "\n",
        "## 코드 구조\n",
        "이 베이스라인 코드는 간단하게 아래 네 단계로 이루어져 있습니다.\n",
        "- `1.세팅` : 필요한 라이브러리 설치, 학습 로그 생성을 위한 전반적인 세팅\n",
        "- `2.데이터`: 사용할 데이터셋을 가져오고 모델에 전달할 Dataloader 생성\n",
        "  - `class CustomDataset`: 데이터를 불러오고 (필요할 경우) 데이터 전처리 진행, 및 torch.utils.data.DataLoader의 첫번째 인자 형식으로 변환\n",
        "  - `torch.utils.data.DataLoader(dataset, batch_size=, ...)`: 모델에 공급할 데이터 로더 생성\n",
        "- `3.모델 설계`: 학습 및 추론에 사용할 모델 구조 설계\n",
        "  - `class TrashClassifier`: 모델 구조 설계\n",
        "- `4.학습`: 설계된 모델로 데이터 학습\n",
        "  - 학습된 가중치 파일은 `.ipynb` 코드와 같은 경로에 .pth 형태로 저장됨\n",
        "- `5.추론`: 학습된 모델을 사용해 테스트 데이터로 추론\n",
        "  - 학습된 모델로 테스트 데이터에 대한 추론을 진행\n",
        "  - 추론 결과는 본 `.ipynb` 코드와 같은 경로에 .csv 형태로 저장됨. 이를 플랫폼에 업로드해 점수 확인"
      ],
      "id": "68e72fd0-ca13-49fc-9e92-bb9c411dacc7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "852384e6-da81-4476-a415-45ddf12cab39"
      },
      "source": [
        "## 세팅\n",
        "### 라이브러리\n",
        "- 코드 전반에 사용되는 라이브러리를 설치 및 로드합니다."
      ],
      "id": "852384e6-da81-4476-a415-45ddf12cab39"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4bc8803-e6ee-47c5-8695-515d1341b7ef"
      },
      "source": [
        "# 설치되지 않은 라이브러리의 경우, 주석 해제 후 코드를 실행하여 설치\n",
        "# !pip install torch\n",
        "# !pip install pandas\n",
        "# !pip install tqdm\n",
        "# !pip install matplotlib\n",
        "# !pip install opencv-python\n",
        "# !pip install sklearn\n",
        "# !pip install pillow"
      ],
      "id": "c4bc8803-e6ee-47c5-8695-515d1341b7ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21ae2ac8-666b-4165-8d4a-621f5917a577"
      },
      "source": [
        "# 필요한 라이브러리 불러오기\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n"
      ],
      "id": "21ae2ac8-666b-4165-8d4a-621f5917a577",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ec81d06-0d9b-4859-9cd3-5156086ed84b"
      },
      "source": [
        "### 기타\n",
        "- SEED 고정 : 시드를 고정하여 실행하면 같은 코드를 여러 번 실행한 결과에 일관성을 부여합니다.\n",
        "- device 설정 : GPU를 사용하기 위해서 지정합니다.\n",
        "- 디렉토리 설정 : 추후 반복적으로 사용하게 될 현재 디렉토리 경로를 설정하세요.\n",
        "  데이터는 현재 디렉토리의 `data/`폴더 안에 저장하세요.  \n",
        "- 셋팅할 working directory 구조  \n",
        "  |--코드.ipynb  \n",
        "  |--data/  \n",
        "  |--|--train/\n",
        "  |--|--|--images/\n",
        "  |--|--|--train.csv  \n",
        "  |--|--test/  \n",
        "  |--|--| "
      ],
      "id": "4ec81d06-0d9b-4859-9cd3-5156086ed84b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5c0be4d-3371-46aa-bf78-da1fdde98e62"
      },
      "source": [
        "# SEED\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# Set device\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"GPU 사용 가능 여부 : \", torch.cuda.is_available())\n",
        "\n",
        "# DEBUG True 설정 시, Performance recorder가 실행되지 않습니다.\n",
        "DEBUG = False\n",
        "\n",
        "# 경로 설정\n",
        "\n",
        "### PROJECT_DIR : 코드를 실행하며 생성될 파일들이 저장될 경로 설정 ###\n",
        "PROJECT_DIR = ''\n",
        "ROOT_PROJECT_DIR = os.path.dirname(PROJECT_DIR)\n",
        "DATA_DIR = 'data/'\n",
        "\n",
        "print(\"project directory 경로 전재 여부 : \", os.path.isdir(PROJECT_DIR))\n",
        "print(\"데이터 경로가 옳은지 확인 : \", os.path.isdir(DATA_DIR))\n",
        "\n"
      ],
      "id": "b5c0be4d-3371-46aa-bf78-da1fdde98e62",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "c620d51f-11c4-4947-abf2-16eee45ae94a"
      },
      "source": [
        "### 학습 로그 기록을 위한 Performance recorder 세팅\n",
        "Performance recorder는 학습 결과를 분석하는 데에 유용한 여러 정보를 저장합니다. 해당 베이스라인에서는 학습을 시작한 시점에 따라 time stamp를 생성, 그에 따라 폴더를 생성하여 Epoch 별로 학습 진행사황 등을 기록합니다."
      ],
      "id": "c620d51f-11c4-4947-abf2-16eee45ae94a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a5c987c-ad4d-4625-a64d-6019dcee98ec"
      },
      "source": [
        "\n",
        "MODEL = 'TrashClassifier'\n",
        "\n",
        "# 학습 시간 (리얼 타임)에 따라 serial 생성\n",
        "KST = timezone(timedelta(hours=9))\n",
        "TRAIN_TIMESTAMP = datetime.now(tz=KST).strftime(\"%Y%m%d%H%M%S\")\n",
        "TRAIN_SERIAL = f'{MODEL}_{TRAIN_TIMESTAMP}' if DEBUG is not True else ''\n",
        "\n",
        "# 학습 performance 기록 경로 설정\n",
        "PERFORMANCE_RECORD_DIR = os.path.join('results', 'train', TRAIN_SERIAL)\n",
        "\n",
        "PERFORMANCE_RECORD_COLUMN_NAME_LIST = ['train_serial', 'train_timestamp', 'model_str', 'optimizer_str', \n",
        "                                       'loss_function_str', 'metric_function_str', 'early_stopping_patience', \n",
        "                                       'batch_size', 'epoch', 'learning_rate', 'momentum', 'random_seed', 'epoch_index', \n",
        "                                       'train_loss', 'validation_loss', 'train_score', 'validation_score', 'elapsed_time']\n"
      ],
      "id": "2a5c987c-ad4d-4625-a64d-6019dcee98ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7d0127a-e777-42d6-860c-618e4643f0c9"
      },
      "source": [
        "# 학습 결과 저장할 폴더 만들기\n",
        "def make_directory(directory: str) -> str:\n",
        "    \"\"\"경로가 없으면 생성\n",
        "    Args:\n",
        "        directory (str): 새로 만들 경로\n",
        "\n",
        "    Returns:\n",
        "        str: 상태 메시지\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        if not os.path.isdir(directory):\n",
        "            os.makedirs(directory)\n",
        "            msg = f\"Directory created {directory}\"\n",
        "\n",
        "        else:\n",
        "            msg = f\"{directory} already exists\"\n",
        "\n",
        "    except OSError as e:\n",
        "        msg = f\"Fail to create directory {directory} {e}\"\n",
        "\n",
        "    return msg\n",
        "\n",
        "make_directory(PERFORMANCE_RECORD_DIR)"
      ],
      "id": "c7d0127a-e777-42d6-860c-618e4643f0c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88de2c74-a92e-46af-a937-e721457232dd"
      },
      "source": [
        "import logging\n",
        "\n",
        "# Set system logger\n",
        "def get_logger(name: str, file_path: str, stream=False) -> logging.RootLogger:\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    formatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s')\n",
        "    stream_handler = logging.StreamHandler()\n",
        "    file_handler = logging.FileHandler(file_path)\n",
        "\n",
        "    stream_handler.setFormatter(formatter)\n",
        "    file_handler.setFormatter(formatter)\n",
        "\n",
        "    if stream:\n",
        "        logger.addHandler(stream_handler)\n",
        "    logger.addHandler(file_handler)\n",
        "\n",
        "    return logger\n",
        "\n",
        "system_logger = get_logger(name='train', file_path=os.path.join(PERFORMANCE_RECORD_DIR, 'train_log.log'))"
      ],
      "id": "88de2c74-a92e-46af-a937-e721457232dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3553fa2a-1579-4897-9a51-c15f82022a6d"
      },
      "source": [
        "## EDA (Explaratory Data Analylsis) - 데이터 확인\n",
        "pandas 라이브러리를 이용해 데이터를 간단하게 살펴보겠습니다.  \n",
        "데이터를 이해하기 위해 더 필요하다고 생각되는 부분을 각자 추가해보세요."
      ],
      "id": "3553fa2a-1579-4897-9a51-c15f82022a6d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8aea77a-7045-40ab-8c1c-4615798d72e2"
      },
      "source": [
        "img = os.listdir(os.path.join(DATA_DIR, 'train', 'images'))\n",
        "print(\"학습할 이미지 갯수 : \", len(img))\n"
      ],
      "id": "c8aea77a-7045-40ab-8c1c-4615798d72e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a6fb4d7-89c9-42b9-b5bf-b4408a997b73"
      },
      "source": [
        "- 이미지 샘플 하나 확인\n"
      ],
      "id": "5a6fb4d7-89c9-42b9-b5bf-b4408a997b73"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2d200ca-6c2b-43fe-94db-796863ca0c8d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from cv2 import cv2\n",
        "\n",
        "sampleImage = os.path.join(DATA_DIR, 'train', 'images',img[4])\n",
        "\n",
        "\n",
        "def getImg(path):\n",
        "    bgr_image = cv2.imread(path)\n",
        "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
        "    return rgb_image\n",
        "\n",
        "sampleImage = getImg(sampleImage)\n",
        "\n",
        "plt.imshow(sampleImage)"
      ],
      "id": "f2d200ca-6c2b-43fe-94db-796863ca0c8d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4baf368e-de47-4f49-9077-e91a47d0590d"
      },
      "source": [
        "- train.csv 파일 확인"
      ],
      "id": "4baf368e-de47-4f49-9077-e91a47d0590d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bbefc46-8800-4841-84c2-3df2433d9d33"
      },
      "source": [
        "train_csv = pd.read_csv(os.path.join(DATA_DIR, 'train', \"train.csv\"))\n",
        "train_csv.sample(n=10)"
      ],
      "id": "0bbefc46-8800-4841-84c2-3df2433d9d33",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adc72177-32ae-42f0-9ff1-b16d643379ed"
      },
      "source": [
        "- 학습 데이터의 쓰레기 분류 분포"
      ],
      "id": "adc72177-32ae-42f0-9ff1-b16d643379ed"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "840a2049-34e6-48d3-aad6-6051b1c30e18"
      },
      "source": [
        "train_csv.groupby('category').count()"
      ],
      "id": "840a2049-34e6-48d3-aad6-6051b1c30e18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3316573e-d2d2-4272-872f-5b0d0e38e6f2"
      },
      "source": [
        "- 카테고리 딕셔너리 생성"
      ],
      "id": "3316573e-d2d2-4272-872f-5b0d0e38e6f2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7d27905-6e98-4ffd-bb59-2cca5e0a45fc"
      },
      "source": [
        "category_list = train_csv['category'].unique()\n",
        "category = {}\n",
        "for i in range(len(category_list)):\n",
        "    category[category_list[i]] = i\n",
        "print(category)"
      ],
      "id": "e7d27905-6e98-4ffd-bb59-2cca5e0a45fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "425d1a65-f926-4977-8d5d-ee939957f3ed"
      },
      "source": [
        "category.keys()"
      ],
      "id": "425d1a65-f926-4977-8d5d-ee939957f3ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "bc0968fe-685f-4079-80dc-35f003943b72"
      },
      "source": [
        "### 데이터 전처리 방법\n",
        "해당 과제는 이미 전처리(resize 등)가 적용된 이미지 데이터를 사용합니다. 해당 베이스라인에서 직접 다루지는 않지만, 이미지 분류 과제에서 자주 사용되는 resize, cubic interpolation 등을 짚고 넘어갑시다.\n",
        "- resize : 이미지 파일의 크기가 너무 커서 학습 시간이 오래걸리거나, 각 이미지 파일의 크기가 다른 점을 보정하기 위해 지정된 규격으로 이미지 크기를 통일합니다. \n",
        "- interpolation : 이미지의 비율을 변경할 때 존재하지 않는 영역에 새로운 픽셀 값을 매핑하거나, 존재하는 픽셀들을 압축하여 새로운 값을 할당해야 합니다. 새로운 픽셀 값은 interpolation(보간법)을 이용하여 구합니다. 여러 보간법이 존재하며, 상황에 따라 다른 보간법을 적용하는 것이 보편적입니다. \n",
        " "
      ],
      "id": "bc0968fe-685f-4079-80dc-35f003943b72"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5231bf0-34ea-4729-bec1-42fdbd911396"
      },
      "source": [
        "## 데이터 로드\n",
        "### 사용할 파라미터\n",
        "- `BATCH_SIZE` : 모델에 입력할 데이터의 단위입니다. 전체 데이터 셋을 여러 개의 소그룹으로 나누어 학습하게 되는데,  batch size는 하나의 소그룹에 속하는 데이터 수가 됩니다 (전체 1000개의 데이터에서 배치 사이즈가 50 이라면 학습 시 1 epoch당 20 iteration을 돌며 전체 데이터를 한번 학습 하게 됩니다).\n"
      ],
      "id": "b5231bf0-34ea-4729-bec1-42fdbd911396"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bcbb550-f251-40ff-a801-5a0ea31cf816"
      },
      "source": [
        "# 데이터로드 파라미터\n",
        "BATCH_SIZE = 128\n",
        "INPUT_SHAPE = (128, 128)\n",
        "\n",
        "# 전체 학습 데이터에서 학습(train), 검증(validation) 셋으로 나눌 때, 검증 셋 비율을 설정합니다.\n",
        "VAL_RATIO = 0.2"
      ],
      "id": "2bcbb550-f251-40ff-a801-5a0ea31cf816",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d90ef1cc-1e4c-4ae0-b4b3-3945fe4a5200"
      },
      "source": [
        "import os\n",
        "import copy\n",
        "from cv2 import cv2\n",
        "import torch\n",
        "import sys\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_dir, mode, input_shape):\n",
        "\n",
        "        # !!!!! 데이터 경로 확인 !!!!!\n",
        "        self.data_dir = os.path.join(data_dir,'train')\n",
        "        self.mode = mode\n",
        "        self.input_shape = input_shape\n",
        "        self.db = self.data_loader()\n",
        "        self.transform = transforms.Compose([transforms.Resize(self.input_shape), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "        self.class_num = len(self.db['label'].unique())\n",
        "        \n",
        "    def data_loader(self):\n",
        "        print('Loading ' + self.mode + ' dataset..')\n",
        "        \n",
        "        img_dir = os.path.join(self.data_dir, 'images')\n",
        "        label_dir = os.path.join(self.data_dir, \"train.csv\")\n",
        "        \n",
        "        # 이미지 폴더 존재 여부 체크\n",
        "        if not os.path.isdir(img_dir):\n",
        "            print(f'!!! Cannot find {img_dir}... !!!')\n",
        "            sys.exit()\n",
        "            \n",
        "        # 라벨 파일 존재 여부 체크\n",
        "        if not os.path.lexists(label_dir):\n",
        "            print(f'!!! Cannot find {label_dir}... !!!')\n",
        "            sys.exit()\n",
        "        \n",
        "        category = {'bundle of ropes': 0, 'circular fish trap': 1, 'eel fish trap': 2, \n",
        "                     'rope': 3, 'fish net': 4, 'other objects': 5, 'rectangular fish trap': 6, \n",
        "                     'tire': 7, 'spring fish trap': 8, 'wood': 9}\n",
        "\n",
        "        category_ls = list(category.keys())\n",
        "        \n",
        "        # train.csv를 pandas로 불러오기\n",
        "        df = pd.read_csv(label_dir)\n",
        "        \n",
        "        # return할 train, val 데이터 초기화 - pandas dataframe 형태로 저장할 예정\n",
        "        db_train = db_val = pd.DataFrame(columns=['img_path','label'])\n",
        "\n",
        "        # 카테고리별로 train/val 비율 맞춰 나누기 위해 카테고리별로 루프 실행\n",
        "        for cat in category_ls:\n",
        "\n",
        "            img_path_list = []\n",
        "            img_label_list = []\n",
        "            \n",
        "            # 특정 카테고리에 해당하는 이미지 리스트 뽑기\n",
        "            img_list = df[\"file_name\"].loc[df['category']==cat].values\n",
        "\n",
        "            for filename in img_list:\n",
        "                if os.path.lexists(os.path.join(img_dir, filename)):\n",
        "                    img_path_list.append(os.path.join(img_dir, filename))\n",
        "                    img_label_list.append(category[cat])\n",
        "\n",
        "                else:\n",
        "                    print(\"cannot find \", os.path.join(img_dir,filename))\n",
        "\n",
        "                    \n",
        "            db = pd.DataFrame({'img_path': img_path_list, 'label': img_label_list}) \n",
        "            \n",
        "            # 학습, 검증 셋 나누기\n",
        "            train, val = train_test_split(db, test_size=VAL_RATIO, random_state=42, shuffle=True)\n",
        "            \n",
        "            db_train = pd.concat([db_train,train])\n",
        "            db_val = pd.concat([db_val,val])\n",
        " \n",
        "        db_train = db_train.sample(frac=1).reset_index(drop=True)\n",
        "        db_val = db_val.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "        if self.mode=='train':\n",
        "            db = db_train\n",
        "\n",
        "        elif self.mode=='val':\n",
        "            db = db_val\n",
        "        else:\n",
        "            print(\"Please check your mode : \", mode, \" must be either train or val\")\n",
        "       \n",
        "        print(db.head())\n",
        "        return db\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.db)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data = copy.deepcopy(self.db.loc[index])\n",
        "\n",
        "        # 1. load image\n",
        "        cvimg = cv2.imread(data['img_path'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
        "        if not isinstance(cvimg, np.ndarray):\n",
        "            raise IOError(\"Fail to read %s\" % data['img_path'])\n",
        "\n",
        "        # 2. preprocessing images\n",
        "        trans_image = self.transform(Image.fromarray(cvimg))\n",
        "        return trans_image, data['label']\n",
        "    \n",
        "    \n",
        "# Load dataset & dataloader\n",
        "train_dataset = CustomDataset(data_dir=DATA_DIR, mode='train', input_shape=INPUT_SHAPE)\n",
        "validation_dataset = CustomDataset(data_dir=DATA_DIR, mode='val', input_shape=INPUT_SHAPE)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "print('\\nTrain set samples:',len(train_dataset),  'Val set samples:', len(validation_dataset))"
      ],
      "id": "d90ef1cc-1e4c-4ae0-b4b3-3945fe4a5200",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "9981c58e-5a81-465a-acf1-a787eda6a067"
      },
      "source": [
        "## 모델 설계 / pretrained model 불러오기\n",
        "### 사용할 파라미터\n",
        "- `LEARNING_RATE` : \n",
        "  - 경사하강법(Gradient Descent)을 통해 loss function의 minimum값을 찾아다닐 때, 그 탐색 과정에 있어서의 보폭 정도로 직관적으로 이해 할 수 있습니다. 보폭이 너무 크다면 최적값을 쉽게 지나칠 위험이 있고, 보폭이 너무 작다면 탐색에 걸리는 시간이 길어집니다.\n",
        "- `EPOCHS` : \n",
        "  - 한 번의 epoch는 인공 신경망에서 전체 데이터 셋에 대해 forward pass/backward pass 과정을 거친 것입니다.\n",
        "  - 즉, epoch이 1만큼 지나면, 전체 데이터 셋에 대해 한번의 학습이 완료된 상태입니다.\n",
        "  - 모델을 만들 때 적절한 epoch 값을 설정해야만 underfitting과 overfitting을 방지할 수 있습니다.\n",
        "  - 1 epoch 에 (데이터 갯수 / batch size) interations 실행\n",
        "- `EARLY_STOPPING_PATIENCE` :\n",
        "  - 너무 많은 epoch은 overfitting을 일으키고, 너무 적은 epoch은 underfitting을 일으킵니다. 이런 딜레마에 빠지지 않기 위도록 특정 시점에 학습을 멈추는 방법이 early stopping입니다.\n",
        "  - 해당 변수는 validation score가 개선되지 않아도 학습을 몇 에폭 더 진행할 지 결정합니다. 예를 들어 EARLY_STOPPING_PATIENCE를 5로 설정하고 validation score가 10 에폭에서 가장 높았지만 다음 에폭부터 줄어든다면, 15에폭까지는 학습을 진행하며 validation score가 더 높아지는지 확인하고, 그렇지 않다면 학습을 중단합니다.\n",
        "- `WEIGHT_DECAY` : \n",
        "  - overfitting을 억제하는 학습 기법의 하나로, 학습된 모델의 복잡도를 줄이기 위해서 학습 중 weight가 너무 큰 값을 가지지 않도록 Loss function에 Weight가 커질 경우에 대한 패널티 항목을 넣습니다."
      ],
      "id": "9981c58e-5a81-465a-acf1-a787eda6a067"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d4f490e-2328-4cb0-9c2d-f9e346834a20"
      },
      "source": [
        "# hyper-parameters\n",
        "LEARNING_RATE = 0.0005\n",
        "EPOCHS = 10\n",
        "\n",
        "EARLY_STOPPING_PATIENCE = 10\n",
        "WEIGHT_DECAY = 0.00001\n",
        "\n",
        "OPTIMIZER = ''\n",
        "SCHEDULER = ''\n",
        "MOMENTUM = ''\n",
        "LOSS_FN = ''\n",
        "METRIC_FN = ''"
      ],
      "id": "4d4f490e-2328-4cb0-9c2d-f9e346834a20",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e769349-03ff-4603-bdfc-ba63d727ee65"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "class TrashClassifier(nn.Module):\n",
        "    def __init__(self, num_class):\n",
        "        super(TrashClassifier, self).__init__()\n",
        "        self.model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_class)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_img):\n",
        "        x = self.model(input_img)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "    \n",
        "# 모델 생성하기\n",
        "model = TrashClassifier(num_class=train_dataset.class_num).to(device)"
      ],
      "id": "0e769349-03ff-4603-bdfc-ba63d727ee65",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49b7597a-a99a-4d85-bb7d-ed55b7de15d4"
      },
      "source": [
        "### 학습에 필요한 함수 설정\n",
        "- `optimizer` : \n",
        "  - 손실함수 값이 최소가 되는 부분을 찾기 위해 학습율과 기울기를 다양하게 수정하여 가중치를 변경시키는 것을 최적화라고 하고, 최적화의 다양한 방식들을 옵티마이저라고 합니다.\n",
        "- `scheduler` : \n",
        "  - Learning rate scheduler는 미리 학습 일정을 정해두고, 그 일정에 따라 학습률을 조정하게 합니다. 다시 말하면 Learning rate가 어떻게 변화하게 할 지 정합니다.\n",
        "- `metric_fn` :\n",
        "  - 학습에서 평가 지표(metric)는 validation에서 훈련된 모델이 얼마나 잘 학습되고 있는지 확인하며, 훈련 과정을 모니터링 하는데 사용합니다. 과제와 학습 모델에 따라 다양한 평가 지표가 사용됩니다.\n",
        "  - 해당 베이스라인에서는 macro f1 score를 평가 지표로 사용합니다."
      ],
      "id": "49b7597a-a99a-4d85-bb7d-ed55b7de15d4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0100cdf5-1b0c-4af7-90cd-69e3eb317b31"
      },
      "source": [
        "import sklearn.metrics as metrics\n",
        "\n",
        "def get_metric_fn(y_pred, y_answer, y_prob):\n",
        "    \"\"\" Metric 함수 반환하는 함수\n",
        "\n",
        "    Returns:\n",
        "        metric_fn (Callable)\n",
        "    \"\"\"\n",
        "    assert len(y_pred) == len(y_answer), 'The size of prediction and answer are not same.'\n",
        "    f1 = metrics.f1_score(y_answer, y_pred,average=\"macro\")\n",
        "    return f1, 1\n",
        "\n",
        "# Set optimizer, scheduler, loss function, metric function, criterion (loss function)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "scheduler =  optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e5, max_lr=0.0001, epochs=EPOCHS, steps_per_epoch=len(train_dataloader))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "metric_fn = get_metric_fn"
      ],
      "id": "0100cdf5-1b0c-4af7-90cd-69e3eb317b31",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "363e3521-7add-4307-8ce5-7f2a38d19a5d"
      },
      "source": [
        "### Trainer 설정\n",
        "- epoch별 학습/검증 절차를 정의하는 Trainer class 입니다."
      ],
      "id": "363e3521-7add-4307-8ce5-7f2a38d19a5d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b45a13d6-79f0-47da-8823-547560e668dc"
      },
      "source": [
        "import tqdm\n",
        "\n",
        "class Trainer():\n",
        "    \"\"\" Trainer\n",
        "        epoch에 대한 학습 및 검증 절차 정의\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, criterion, model, device, metric_fn, optimizer=None, scheduler=None, logger=None):\n",
        "        \"\"\" 초기화\n",
        "        \"\"\"\n",
        "        self.criterion = criterion\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.optimizer = optimizer\n",
        "        self.logger = logger\n",
        "        self.scheduler = scheduler\n",
        "        self.metric_fn = metric_fn\n",
        "\n",
        "    def train_epoch(self, dataloader, epoch_index):\n",
        "        \"\"\" 한 epoch에서 수행되는 학습 절차\n",
        "\n",
        "        Args:\n",
        "            dataloader (`dataloader`)\n",
        "            epoch_index (int)\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        train_total_loss = 0\n",
        "        target_lst = []\n",
        "        pred_lst = []\n",
        "        prob_lst = []\n",
        "        for batch_index, (img, label) in enumerate(dataloader):\n",
        "            \n",
        "            img = img.to(self.device)\n",
        "            label = label.to(self.device).long()\n",
        "            pred = self.model(img)\n",
        "            loss = self.criterion(pred, label)\n",
        "            \n",
        "            ## Pytorch에서는 gradients값들을 추후에 backward를 해줄때 계속 더해주기 때문에 \n",
        "            ## 항상 backpropagation을 하기전에 gradients를 zero로 만들어주고 시작을 해야합니다.\n",
        "            self.optimizer.zero_grad()\n",
        "            \n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step()\n",
        "            train_total_loss += loss.item()\n",
        "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
        "            target_lst.extend(label.cpu().tolist())\n",
        "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
        "        self.train_mean_loss = train_total_loss / batch_index\n",
        "        self.train_score, auroc = self.metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
        "        msg = f'Epoch {epoch_index}, Train loss: {self.train_mean_loss}, Acc: {self.train_score}'\n",
        "        print(msg)\n",
        "        #self.logger.info(msg) if self.logger else print(msg)\n",
        "\n",
        "    def validate_epoch(self, dataloader, epoch_index, mode=None):\n",
        "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
        "\n",
        "        Args:\n",
        "            dataloader (`dataloader`)\n",
        "            epoch_index (int)\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        val_total_loss = 0\n",
        "        target_lst = []\n",
        "        pred_lst = []\n",
        "        prob_lst = []\n",
        "        with torch.no_grad():\n",
        "            for batch_index, (img, label) in enumerate(dataloader):\n",
        "                img = img.to(self.device)\n",
        "                label = label.to(self.device).long()\n",
        "                pred = self.model(img)\n",
        "                loss = self.criterion(pred, label)\n",
        "                val_total_loss += loss.item()\n",
        "                prob_lst.extend(pred[:, 1].cpu().tolist())\n",
        "                target_lst.extend(label.cpu().tolist())\n",
        "                pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
        "            self.val_mean_loss = val_total_loss / batch_index\n",
        "            self.validation_score, auroc = self.metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
        "            msg = f'Epoch {epoch_index}, {mode} loss: {self.val_mean_loss}, Acc: {self.validation_score}, ROC: {auroc}'\n",
        "            print(msg)\n",
        "        #self.logger.info(msg) if self.logger else print(msg)\n",
        "\n",
        "        \n",
        "# Set trainer\n",
        "trainer = Trainer(criterion, model, device, metric_fn, optimizer, scheduler, logger=system_logger)"
      ],
      "id": "b45a13d6-79f0-47da-8823-547560e668dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23c8f87c-eb11-4413-9a8b-02ab2c6590b9"
      },
      "source": [
        "### Earlystopper 세팅"
      ],
      "id": "23c8f87c-eb11-4413-9a8b-02ab2c6590b9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "63d19482-2c77-48ab-962c-b4409c4c4a7c"
      },
      "source": [
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "class LossEarlyStopper():\n",
        "    \"\"\"Early stopper\n",
        "    \n",
        "    Attributes:\n",
        "        patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
        "        verbose (bool): 로그 출력 여부, True 일 때 로그 출력\n",
        "        patience_counter (int): loss 가 줄어들지 않을 때 마다 1씩 증가\n",
        "        min_loss (float): 최소 loss\n",
        "        stop (bool): True 일 때 학습 중단\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patience: int, verbose: bool, logger:logging.RootLogger=None)-> None:\n",
        "        \"\"\" 초기화\n",
        "\n",
        "        Args:\n",
        "            patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
        "            weight_path (str): weight 저장경로\n",
        "            verbose (bool): 로그 출력 여부, True 일 때 로그 출력\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.patience_counter = 0\n",
        "        self.min_loss = np.Inf\n",
        "        self.logger = logger\n",
        "        self.stop = False\n",
        "\n",
        "    def check_early_stopping(self, loss: float)-> None:\n",
        "        \"\"\"Early stopping 여부 판단\n",
        "\n",
        "        Args:\n",
        "            loss (float):\n",
        "\n",
        "        Examples:\n",
        "            \n",
        "        Note:\n",
        "            \n",
        "        \"\"\"  \n",
        "\n",
        "        if self.min_loss == np.Inf:\n",
        "            #첫 에폭\n",
        "            self.min_loss = loss\n",
        "            # self.save_checkpoint(loss=loss, model=model)\n",
        "\n",
        "        elif loss > self.min_loss:\n",
        "            # loss가 줄지 않음 -> patience_counter 1 증가\n",
        "            self.patience_counter += 1\n",
        "            msg = f\"Early stopper, Early stopping counter {self.patience_counter}/{self.patience}\"\n",
        "\n",
        "            if self.patience_counter == self.patience:\n",
        "                self.stop = True\n",
        "\n",
        "            if self.verbose:\n",
        "                self.logger.info(msg) if self.logger else print(msg)\n",
        "                \n",
        "        elif loss <= self.min_loss:\n",
        "            # loss가 줄어듬 -> min_loss 갱신\n",
        "            self.save_model = True\n",
        "            msg = f\"Early stopper, Validation loss decreased {self.min_loss} -> {loss}\"\n",
        "            self.min_loss = loss\n",
        "            # self.save_checkpoint(loss=loss, model=model)\n",
        "\n",
        "            if self.verbose:\n",
        "                self.logger.info(msg) if self.logger else print(msg)\n",
        "                \n",
        "# Set earlystopper\n",
        "early_stopper = LossEarlyStopper(patience=EARLY_STOPPING_PATIENCE, verbose=True, logger=system_logger)\n"
      ],
      "id": "63d19482-2c77-48ab-962c-b4409c4c4a7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "98a5618a-248d-48ca-8cb0-cb2cb24d887c"
      },
      "source": [
        "### Performance Recorder 세팅"
      ],
      "id": "98a5618a-248d-48ca-8cb0-cb2cb24d887c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "22129444-4863-44bc-b493-ebed23d8ddbe"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import logging\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class PerformanceRecorder():\n",
        "\n",
        "    def __init__(self,\n",
        "                 column_name_list: list,\n",
        "                 record_dir: str,\n",
        "                 key_column_value_list: list,\n",
        "                 model: 'model',\n",
        "                 optimizer: 'optimizer',\n",
        "                 scheduler: 'scheduler',\n",
        "                 logger: logging.RootLogger=None):\n",
        "        \"\"\"Recorder 초기화\n",
        "            \n",
        "        Args:\n",
        "            column_name_list (list(str)):\n",
        "            record_dir (str):\n",
        "            key_column_value_list (list)\n",
        "\n",
        "        Note:\n",
        "        \"\"\"\n",
        "\n",
        "        self.column_name_list = column_name_list\n",
        "        \n",
        "        self.record_dir = record_dir\n",
        "        self.record_filepath = os.path.join(self.record_dir, 'record.csv')\n",
        "        self.best_record_filepath = os.path.join('/'.join(self.record_dir.split('/')[:-1]),'train_best_record.csv')\n",
        "        self.weight_path = os.path.join(record_dir, 'model.pt')\n",
        "\n",
        "        self.row_counter = 0\n",
        "        self.key_column_value_list = key_column_value_list\n",
        "\n",
        "        self.train_loss_list = list()\n",
        "        self.validation_loss_list= list()\n",
        "        self.train_score_list = list()\n",
        "        self.validation_score_list = list()\n",
        "\n",
        "        self.loss_plot = None\n",
        "        self.score_plot = None\n",
        "\n",
        "        self.min_loss = np.Inf\n",
        "        self.best_record = None\n",
        "\n",
        "        self.logger = logger\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "        self.key_column_value_list = key_column_value_list\n",
        "\n",
        "    def set_model(self, model: 'model'):\n",
        "        self.model = model\n",
        "\n",
        "    def set_logger(self, logger: logging.RootLogger):\n",
        "        self.logger = logger\n",
        "\n",
        "    def create_record_directory(self):\n",
        "        \"\"\"\n",
        "        record 경로 생성\n",
        "        \"\"\"\n",
        "        make_directory(self.record_dir)\n",
        "        msg = f\"Create directory {self.record_dir}\"\n",
        "        self.logger.info(msg) if self.logger else None\n",
        "\n",
        "    def add_row(self,\n",
        "                epoch_index: int,\n",
        "                train_loss: float,\n",
        "                validation_loss: float,\n",
        "                train_score: float,\n",
        "                validation_score: float):\n",
        "        \"\"\"Epoch 단위 성능 적재\n",
        "        \n",
        "        최고 성능 Epoch 모니터링\n",
        "        모든 Epoch 종료 이후 최고 성능은 train_best_records.csv 에 적재\n",
        "\n",
        "        Args:\n",
        "            row (list): \n",
        "\n",
        "        \"\"\"\n",
        "        self.train_loss_list.append(train_loss)\n",
        "        self.validation_loss_list.append(validation_loss)\n",
        "        self.train_score_list.append(train_score)\n",
        "        self.validation_score_list.append(validation_score)\n",
        "\n",
        "        row = self.key_column_value_list + [epoch_index, train_loss, validation_loss, train_score, validation_score]\n",
        "        \n",
        "        # Update scheduler learning rate\n",
        "        learning_rate = self.scheduler.get_last_lr()[0]\n",
        "        row[9] = learning_rate\n",
        "\n",
        "        with open(self.record_filepath, newline='', mode='a') as f:\n",
        "            writer = csv.writer(f)\n",
        "\n",
        "            if self.row_counter == 0:\n",
        "                writer.writerow(self.column_name_list)\n",
        "\n",
        "            writer.writerow(row)\n",
        "            msg = f\"Write row {self.row_counter}\"\n",
        "            self.logger.info(msg) if self.logger else None\n",
        "\n",
        "        self.row_counter += 1\n",
        "\n",
        "        # Update best record & Save check point\n",
        "        if validation_loss < self.min_loss:\n",
        "            msg = f\"Update best record row {self.row_counter}, checkpoints {self.min_loss} -> {validation_loss}\"\n",
        "            \n",
        "            self.min_loss = validation_loss\n",
        "            self.best_record = row\n",
        "            self.save_weight()\n",
        "\n",
        "            self.logger.info(msg) if self.logger else None\n",
        "\n",
        "    def add_best_row(self):\n",
        "        \"\"\"\n",
        "        모든 Epoch 종료 이후 최고 성능에 해당하는 row을 train_best_records.csv 에 적재\n",
        "        \"\"\"\n",
        "\n",
        "        n_row = count_csv_row(self.best_record_filepath)\n",
        "\n",
        "        with open(self.best_record_filepath, newline='', mode='a') as f:\n",
        "            writer = csv.writer(f)\n",
        "\n",
        "            if n_row == 0:\n",
        "               writer.writerow(self.column_name_list)\n",
        "            \n",
        "            writer.writerow(self.best_record)\n",
        "\n",
        "        msg = f\"Save best record {self.best_record_filepath}\"\n",
        "        self.logger.info(msg) if self.logger else None\n",
        "\n",
        "    def save_weight(self)-> None:\n",
        "        \"\"\"Weight 저장\n",
        "\n",
        "        Args:\n",
        "            loss (float): validation loss\n",
        "            model (`model`): model\n",
        "        \n",
        "        \"\"\"\n",
        "        check_point = {\n",
        "            'model': self.model.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'scheduler': self.scheduler.state_dict()\n",
        "        }\n",
        "        torch.save(check_point, self.weight_path)\n",
        "        #torch.save(self.model.state_dict(), self.weight_path)\n",
        "        msg = f\"Model saved: {self.weight_path}\"\n",
        "        self.logger.info(msg) if self.logger else None\n",
        "\n",
        "    def save_performance_plot(self, final_epoch: int):\n",
        "        \"\"\"Epoch 단위 loss, score plot 생성 후 저장\n",
        "\n",
        "        \"\"\"\n",
        "        self.loss_plot = self.plot_performance(epoch=final_epoch+1,\n",
        "                                          train_history=self.train_loss_list,\n",
        "                                          validation_history=self.validation_loss_list,\n",
        "                                          target='loss')\n",
        "        self.score_plot = self.plot_performance(epoch=final_epoch+1,\n",
        "                                           train_history=self.train_score_list,\n",
        "                                           validation_history=self.validation_score_list,\n",
        "                                           target='score')\n",
        "\n",
        "        self.loss_plot.savefig(os.path.join(self.record_dir, 'loss.png'))\n",
        "        self.score_plot.savefig(os.path.join(self.record_dir, 'score.jpg'))\n",
        "        plt.close('all')\n",
        "\n",
        "\n",
        "        msg = f\"Save performance plot {self.record_dir}\"\n",
        "        self.logger.info(msg) if self.logger else None\n",
        "\n",
        "    def plot_performance(self,\n",
        "                         epoch: int,\n",
        "                         train_history:list,\n",
        "                         validation_history:list,\n",
        "                         target:str)-> plt.figure:\n",
        "        \"\"\"loss, score plot 생성\n",
        "\n",
        "        \"\"\"\n",
        "        fig = plt.figure(figsize=(12, 5))\n",
        "        epoch_range = list(range(epoch))\n",
        "\n",
        "        plt.plot(epoch_range, train_history, marker='.', c='red', label=\"train\")\n",
        "        plt.plot(epoch_range, validation_history, marker='.', c='blue', label=\"validation\")\n",
        "\n",
        "        plt.legend(loc='upper right')\n",
        "        plt.grid()\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel(target)\n",
        "\n",
        "        return fig"
      ],
      "id": "22129444-4863-44bc-b493-ebed23d8ddbe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "f4aaa352-e86e-48a2-9781-36cf2088fabc"
      },
      "source": [
        "key_column_value_list = [\n",
        "    TRAIN_SERIAL,\n",
        "    TRAIN_TIMESTAMP,\n",
        "    MODEL,\n",
        "    OPTIMIZER,\n",
        "    LOSS_FN,\n",
        "    METRIC_FN,\n",
        "    EARLY_STOPPING_PATIENCE,\n",
        "    BATCH_SIZE,\n",
        "    EPOCHS,\n",
        "    LEARNING_RATE,\n",
        "    WEIGHT_DECAY,\n",
        "    RANDOM_SEED]\n",
        "\n",
        "performance_recorder = PerformanceRecorder(column_name_list=PERFORMANCE_RECORD_COLUMN_NAME_LIST,\n",
        "                                           record_dir=PERFORMANCE_RECORD_DIR,\n",
        "                                           key_column_value_list=key_column_value_list,\n",
        "                                           logger=system_logger,\n",
        "                                           model=model,\n",
        "                                           optimizer=optimizer,\n",
        "                                           scheduler=scheduler)\n"
      ],
      "id": "f4aaa352-e86e-48a2-9781-36cf2088fabc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd861d4a-c206-4bc6-96ba-d64b127c422e"
      },
      "source": [
        "### 학습\n",
        "- 각 에폭별 수행할 절차를 trainer 클래스의 train_epoch, validation_epoch을 이용해 정의합니다.  \n",
        "- EPOCHS만큼, 혹은 early_stop이 될 때까지 에폭을 반복합니다."
      ],
      "id": "dd861d4a-c206-4bc6-96ba-d64b127c422e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ffc140b-3227-45fc-9dae-1f5f9367bd5c"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "criterion = 1E+8\n",
        "\n",
        "# 저장될 모델 이름\n",
        "model_name = 'best.pt'\n",
        "\n",
        "# 에폭 별로 train_epoch, valid_epoch 실헹힘\n",
        "for epoch_index in tqdm(range(EPOCHS)):\n",
        "\n",
        "    trainer.train_epoch(train_dataloader, epoch_index)\n",
        "    trainer.validate_epoch(validation_dataloader, epoch_index, 'val')\n",
        "\n",
        "    # Performance record - csv & save elapsed_time\n",
        "    performance_recorder.add_row(epoch_index=epoch_index,\n",
        "                                 train_loss=trainer.train_mean_loss,\n",
        "                                 validation_loss=trainer.val_mean_loss,\n",
        "                                 train_score=trainer.train_score,\n",
        "                                 validation_score=trainer.validation_score)\n",
        "\n",
        "    # Performance record - plot\n",
        "    performance_recorder.save_performance_plot(final_epoch=epoch_index)\n",
        "\n",
        "    # early_stopping check\n",
        "    early_stopper.check_early_stopping(loss=trainer.val_mean_loss)\n",
        "\n",
        "    if early_stopper.stop:\n",
        "        print('Early stopped')\n",
        "        break\n",
        "    \n",
        "    if trainer.val_mean_loss < criterion:\n",
        "        criterion = trainer.val_mean_loss\n",
        "        performance_recorder.weight_path = os.path.join(PERFORMANCE_RECORD_DIR, model_name)\n",
        "        performance_recorder.save_weight()\n",
        "        # print(f'{epoch_index} model saved train acc :{trainer.train_score}, val acc :{trainer.validation_score}')\n",
        "        print(f'{epoch_index} model saved')\n",
        "        print('----------------------------------')\n",
        "\n",
        "\n",
        "print(\"Model saved: \", os.path.join(PERFORMANCE_RECORD_DIR, model_name))\n",
        "print(\"Train score saved: \",os.path.join(PERFORMANCE_RECORD_DIR, 'score.jpg'))\n",
        "print(\"Train loss saved: \",os.path.join(PERFORMANCE_RECORD_DIR, 'loss.png'))"
      ],
      "id": "9ffc140b-3227-45fc-9dae-1f5f9367bd5c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "0261645e-f61f-477a-be90-676833c1419e"
      },
      "source": [
        "## 추론 (Predict)\n",
        "테스트 데이터의 타겟 변수를 `sample_submission.csv` 양식에 맞춰 저장한 파일을 인공지능 놀이터 플랫폼을 통해 제출하면 추론 점수를 확인할 수 있습니다.\n",
        "\n",
        "\"category\" 컬럼 값을 여러분의 모델의 추론 결과로 채워 제출 파일을 만듭니다 (현재는 모두 아래 보시는 바와 같이 동일한 값으로 채워져 있습니다). \"file_name\" 값을 기준으로 채점을 진행하는 점 유의해주시기 바랍니다."
      ],
      "id": "0261645e-f61f-477a-be90-676833c1419e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e32e63a3-fbce-4a0d-89f6-8ae2906bd438"
      },
      "source": [
        "### 제출 파일 양식 확인"
      ],
      "id": "e32e63a3-fbce-4a0d-89f6-8ae2906bd438"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abe06081-93b0-4d34-a4d8-216a98e8434a"
      },
      "source": [
        "sample_submission = pd.read_csv(\"sample_submission.csv\")"
      ],
      "id": "abe06081-93b0-4d34-a4d8-216a98e8434a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e64a718-21d1-43f0-a79d-4eb3e66a8c59"
      },
      "source": [
        "sample_submission.head()"
      ],
      "id": "4e64a718-21d1-43f0-a79d-4eb3e66a8c59",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e627956-cec3-4494-b977-0d8ee9afa16a"
      },
      "source": [
        "### 추론을 위한 라이브러리 불러오기 및 세팅 "
      ],
      "id": "5e627956-cec3-4494-b977-0d8ee9afa16a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89e70306-3fbb-4cc6-9364-94e47c3e18e6"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import cv2\n",
        "import sys\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# Set device\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "\n",
        "# # CONFIG\n",
        "PROJECT_DIR = ''\n",
        "ROOT_PROJECT_DIR = os.path.dirname(PROJECT_DIR)\n",
        "DATA_DIR = 'data/'\n",
        "\n",
        "# SEED\n",
        "RANDOM_SEED = 42\n"
      ],
      "id": "89e70306-3fbb-4cc6-9364-94e47c3e18e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61218725-8013-415b-bcf0-1dc5d0f8a91e"
      },
      "source": [
        "# train 데이터로부터 카테고리 갯수 가져와 지정 \n",
        "\n",
        "train_csv = pd.read_csv(os.path.join(DATA_DIR, 'train', \"train.csv\"))\n",
        "\n",
        "# 카테고리 갯수 = (분류) class 갯수 \n",
        "num_category = len(train_csv['category'].unique())"
      ],
      "id": "61218725-8013-415b-bcf0-1dc5d0f8a91e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaf1d144-6516-4712-8de1-e2c5f7117fd8"
      },
      "source": [
        "# PREDICT\n",
        "BATCH_SIZE = 16\n",
        "INPUT_SHAPE = (128, 128)\n",
        "NUM_CLASS = num_category\n"
      ],
      "id": "eaf1d144-6516-4712-8de1-e2c5f7117fd8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d92ce28e-e283-40d7-b867-48c58a75f487"
      },
      "source": [
        "# 시드 고정\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# 평가 지표 함수, criterion (loss function) 설정\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "def get_metric_fn(y_pred, y_answer, y_prob):\n",
        "    \"\"\" Metric 함수 반환하는 함수\n",
        "\n",
        "    Returns:\n",
        "        metric_fn (Callable)\n",
        "    \"\"\"\n",
        "    assert len(y_pred) == len(y_answer), 'The size of prediction and answer are not same.'\n",
        "    f1 = metrics.f1_score(y_answer, y_pred,average=\"macro\")\n",
        "    return f1, 1\n",
        "\n",
        "metric_fn = get_metric_fn\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "id": "d92ce28e-e283-40d7-b867-48c58a75f487",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4149693-b9bb-4a43-a1e2-23b8d2ea2a7b"
      },
      "source": [
        "### 테스트 데이터 로드"
      ],
      "id": "c4149693-b9bb-4a43-a1e2-23b8d2ea2a7b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e493f756-c235-475c-9951-6e5eb944e89e"
      },
      "source": [
        "# TestDataset 만드는 클래스\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, data_dir, input_shape):\n",
        "        self.data_dir = os.path.join(data_dir,'test')\n",
        "        self.input_shape = input_shape\n",
        "        self.db = self.data_loader()\n",
        "        self.transform = transforms.Compose([transforms.Resize(self.input_shape), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "        \n",
        "    def data_loader(self):\n",
        "        print('Loading test dataset..')\n",
        "        if not os.path.isdir(self.data_dir):\n",
        "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
        "            sys.exit()\n",
        "        \n",
        "        root = os.path.join(self.data_dir, 'images')\n",
        "        \n",
        "        if os.path.isdir(root):\n",
        "            img_list = os.listdir(root)\n",
        "        else:\n",
        "            print(root, \" doesn't exist\")\n",
        "        img_path_list = []\n",
        "        \n",
        "        for filename in img_list:\n",
        "            ext = os.path.splitext(filename)[-1]\n",
        "            if (os.path.lexists(os.path.join(root, filename))):\n",
        "                if '.jpg' == ext:\n",
        "                     img_path_list.append(os.path.join(root, filename))\n",
        "            else:\n",
        "                print(\"please check your test set directory .... \")\n",
        "                break\n",
        "\n",
        "        db = pd.DataFrame({'img_path': img_path_list})\n",
        "        \n",
        "        print(db.sample(n=5))\n",
        "        print(\"Train set samples: \", len(db))\n",
        "        \n",
        "        return db\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.db)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        data = copy.deepcopy(self.db.loc[index])\n",
        "        \n",
        "         #1. load image\n",
        "        cvimg = cv2.imread(data['img_path'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
        "        if not isinstance(cvimg, np.ndarray):\n",
        "            raise IOError(\"Fail to read %s\" % data['img_path'])\n",
        "\n",
        "        # 2. preprocessing images\n",
        "        trans_image = self.transform(Image.fromarray(cvimg))\n",
        "        return trans_image, data['img_path'].split('/')[-1]\n"
      ],
      "id": "e493f756-c235-475c-9951-6e5eb944e89e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f812e45-c2e4-46a7-8c18-55cedcd6907e"
      },
      "source": [
        "# 테스트 데이터 로드   \n",
        "test_dataset = TestDataset(data_dir=DATA_DIR, input_shape=INPUT_SHAPE)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "id": "2f812e45-c2e4-46a7-8c18-55cedcd6907e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e74cfef-adb2-4289-b204-287d26e31986"
      },
      "source": [
        "### 학습한 가중치 파일 불러오기"
      ],
      "id": "4e74cfef-adb2-4289-b204-287d26e31986"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "c6cb42eb-a639-4ef9-a3aa-bd3d63f9f913"
      },
      "source": [
        "# pre-trained model 함수\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "class TrashClassifier(nn.Module):\n",
        "    def __init__(self, num_class):\n",
        "        super(TrashClassifier, self).__init__()\n",
        "        self.model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_class)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_img):\n",
        "        x = self.model(input_img)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ],
      "id": "c6cb42eb-a639-4ef9-a3aa-bd3d63f9f913",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c29a2fe8-f2c1-453f-99bf-468912bd26cf"
      },
      "source": [
        "# 앞서 모델이 저장된 경로를 입력하세요\n",
        "# 예시 :  'results/train/TrashClassifier_202111025172155/best.pt'\n",
        "TRAINED_MODEL_PATH = ###\n",
        "\n",
        "print(\"학습한 모델 weight 파일이 경로에 존재하나요? - \", os.path.lexists(TRAINED_MODEL_PATH))\n",
        "model = TrashClassifier(num_class=NUM_CLASS).to(device)\n",
        "model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'])\n"
      ],
      "id": "c29a2fe8-f2c1-453f-99bf-468912bd26cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67c5c6aa-cb13-41c6-81d1-7c3c6e256037"
      },
      "source": [
        "### Predict ! (test trainer)\n",
        "- 분류 카테고리에 0부터 9까지의 정수 코드를 부여해 학습하였으므로 추론된 output도 정수일 것입니다. 이를 다시 단어로 바꿔주는 디코딩 작업도 잊지 마세요."
      ],
      "id": "67c5c6aa-cb13-41c6-81d1-7c3c6e256037"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "955fd08d-a7d1-4ece-99b4-c4f93735ba03"
      },
      "source": [
        "# 제출할 submission.csv 파일명\n",
        "SAVE_PATH = 'submission.csv'\n",
        "\n",
        "# 디코딩을 위한 딕셔너리\n",
        "category = {'bundle of ropes': 0, 'circular fish trap': 1, 'eel fish trap': 2, \n",
        "             'rope': 3, 'fish net': 4, 'other objects': 5, 'rectangular fish trap': 6, \n",
        "             'tire': 7, 'spring fish trap': 8, 'wood': 9}\n",
        "decode = dict(map(reversed, category.items()))\n",
        "\n",
        "\n",
        "file_name_lst = []\n",
        "pred_lst = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_index, (img, file_name) in enumerate(test_dataloader):\n",
        "        img = img.to(device)\n",
        "        pred = model(img)\n",
        "        pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
        "        file_name_lst.extend(file_name)\n",
        "print(\"Prediction completed\")        \n",
        "\n",
        "# 디코딩\n",
        "pred_lst = list(map(lambda x: decode[x], pred_lst))\n",
        "print(\"decoding completed\")"
      ],
      "id": "955fd08d-a7d1-4ece-99b4-c4f93735ba03",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a510b3d7-0fe5-40eb-8f9d-c2f83a47b588"
      },
      "source": [
        "### 결과 제출 파일 확인"
      ],
      "id": "a510b3d7-0fe5-40eb-8f9d-c2f83a47b588"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3b43b95-4ac3-4053-b01a-871e7504c9d2"
      },
      "source": [
        "pred = pd.DataFrame({'file_name':file_name_lst, 'category':pred_lst})\n",
        "pred.head()"
      ],
      "id": "f3b43b95-4ac3-4053-b01a-871e7504c9d2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a0ceda4-fd1f-4634-9957-732ef1eb0462"
      },
      "source": [
        "### 결과 제출 파일 만들기"
      ],
      "id": "6a0ceda4-fd1f-4634-9957-732ef1eb0462"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cdadcf4-fbeb-4abd-8f2b-ea6af2b680dc"
      },
      "source": [
        "pred.to_csv(SAVE_PATH, index=False)\n",
        "print(SAVE_PATH, \"saved.\")"
      ],
      "id": "0cdadcf4-fbeb-4abd-8f2b-ea6af2b680dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93d9a7d6-6195-4fe1-b134-5027c9c6357e"
      },
      "source": [
        "-> 이 파일을 홈페이지에 업로드해 점수를 확인해보세요!"
      ],
      "id": "93d9a7d6-6195-4fe1-b134-5027c9c6357e"
    }
  ]
}