{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Trash.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ming-90/Kaggle_Study/blob/main/5.%20TrashClassifier/Trash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ada645-5ee8-4ad5-857e-dc90fa08970a"
      },
      "source": [
        "#[이미지 분류] 수중 해양 쓰레기 종류 분류 모델"
      ],
      "id": "d7ada645-5ee8-4ad5-857e-dc90fa08970a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa3fpHKUJgNl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "9d685918-72b5-470a-e03e-2d7e7a366e0d"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"drive\")"
      ],
      "id": "Oa3fpHKUJgNl",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-6143e07d9951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0muse_metadata_server\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_metadata_server\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       ephemeral=ephemeral)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server, ephemeral)\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;34m': timeout during initial read of root folder; for more info: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             'https://research.google.com/colaboratory/faq.html#drive-timeout')\n\u001b[0;32m--> 286\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m       \u001b[0;31m# Not already authorized, so do the authorization dance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0tilKTHK1Bx"
      },
      "source": [
        "cd /content/drive/MyDrive/Colab\\ Notebooks/Trash"
      ],
      "id": "L0tilKTHK1Bx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfL2YR2XaO-r"
      },
      "source": [
        "ls"
      ],
      "id": "GfL2YR2XaO-r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKywah66g6t-"
      },
      "source": [
        "## 1. 세팅 : 필요한 라이브러리 설치, 학습 로그 생성을 위한 전반적인 셋팅"
      ],
      "id": "vKywah66g6t-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcxEAw8GhSo3"
      },
      "source": [
        "import os\n",
        "import copy\n",
        "import sys\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime, timezone, timedelta\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from cv2 import cv2\n",
        "from PIL import Image"
      ],
      "id": "LcxEAw8GhSo3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK7b1xE9jd8L"
      },
      "source": [
        "### 기타\n",
        " * SEED 고정 : 시드를 고정하여 실행하면 같은 코드를 여러번 실행한 결과에 일관성 부여\n",
        " * device 설정 : GPU를 사용하기 위해"
      ],
      "id": "tK7b1xE9jd8L"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyqLg4CTjwo0"
      },
      "source": [
        "# SEED\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# Set Divece\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"GPU 사용 가능 여부 : \", torch.cuda.is_available())\n",
        "\n",
        "# DEBUG True 설정 시, Performance recorder가 실행 안됨\n",
        "DEBUG = False\n",
        "\n",
        "# 경로 설정\n",
        "PROJECT_DIR = './'\n",
        "ROOT_PROJECT_DIR = os.path.dirname(PROJECT_DIR)\n",
        "DATA_DIR = 'data/'\n",
        "\n",
        "print(\"project directory 경로 존재 여부 : \", os.path.isdir(PROJECT_DIR))\n",
        "print(\"경로\", os.path.isdir(DATA_DIR))"
      ],
      "id": "UyqLg4CTjwo0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZDhyGm3l2Bf"
      },
      "source": [
        "### 학습 로그 기록을 위한 Performance recorder 셋팅\n",
        "#### Performance recorder는 학습 결과를 분석하는 데 유용한 여러 정보를 저장."
      ],
      "id": "8ZDhyGm3l2Bf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiDD5xL7mGb0"
      },
      "source": [
        "MODEL = \"TrashClassifier\"\n",
        "\n",
        "# 학습 시간에 따라 serial 생성\n",
        "KST = timezone(timedelta(hours=9))\n",
        "TRAIN_TIMESTAMP = datetime.now(tz=KST).strftime(\"%Y%m%d%H%M%S\")\n",
        "TRAIN_SERIAL = f'{MODEL}_{TRAIN_TIMESTAMP}' if DEBUG is not True else ''\n",
        "\n",
        "# 학습 performance 기록 경로 설정\n",
        "PERFORMANCE_RECORD_DIR = os.path.join('results','train', TRAIN_SERIAL)\n",
        "PERFORMANCE_RECORD_COLUMN_NAME_LIST = ['train_serial', 'train_timestamp', 'model_str', 'optimizer_str', \n",
        "                                       'loss_function_str', 'metric_function_str', 'early_stopping_patience', \n",
        "                                       'batch_size', 'epoch', 'learning_rate', 'momentum', 'random_seed', 'epoch_index', \n",
        "                                       'train_loss', 'validation_loss', 'train_score', 'validation_score', 'elapsed_time']"
      ],
      "id": "BiDD5xL7mGb0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXx0weUDoncf"
      },
      "source": [
        "# 학습 결과 저장할 폴더 만들기\n",
        "\n",
        "def make_directory(directory: str) -> str:\n",
        "    try:\n",
        "        if not os.path.isdir(directory):\n",
        "            os.makedirs(directory)\n",
        "            msg = f'Directory created {directory}'\n",
        "        else:\n",
        "            msg = f'{directory} already exists'\n",
        "    except OSError as e:\n",
        "        mas = f'Fail to create directory {directory} {e}'\n",
        "        \n",
        "    return msg\n",
        "make_directory(PERFORMANCE_RECORD_DIR)"
      ],
      "id": "JXx0weUDoncf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_02dO3RotfL"
      },
      "source": [
        "import logging\n",
        "\n",
        "def get_logger(name: str, file_path: str, stream=False) -> logging.RootLogger:\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    formatter = logging.Formatter(\"%(asctime)s | %(name)s | %(levelname)s | %(message)s\")\n",
        "    stream_handler = logging.StreamHandler()\n",
        "    file_handler = logging.FileHandler(file_path)\n",
        "\n",
        "    stream_handler.setFormatter(formatter)\n",
        "    file_handler.setFormatter(formatter)\n",
        "\n",
        "    if stream:\n",
        "        logger.addHandler(stream_handler)\n",
        "    logger.addHandler(file_handler)\n",
        "\n",
        "    return logger\n",
        "\n",
        "system_logger = get_logger(name='train', file_path=os.path.join(PERFORMANCE_RECORD_DIR, 'train_log.log'))"
      ],
      "id": "e_02dO3RotfL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC7Ye-u4qX9B"
      },
      "source": [
        "## 2. EDA\n",
        "#### pandas 라이브러리를 이용해 데이터 확인"
      ],
      "id": "GC7Ye-u4qX9B"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb_AG09HqiYC"
      },
      "source": [
        "img = os.listdir(os.path.join(DATA_DIR, 'train', 'images'))\n",
        "print(\"이미지 갯수 : \", len(img))"
      ],
      "id": "Wb_AG09HqiYC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRykEjrNrhiG"
      },
      "source": [
        " * 이미지 샘플 확인"
      ],
      "id": "ZRykEjrNrhiG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WP7fsXtq_N-"
      },
      "source": [
        "\n",
        "sampleImage = os.path.join(DATA_DIR, 'train', 'images', img[4])\n",
        "\n",
        "def getImg(path):\n",
        "    bgr_image = cv2.imread(path)\n",
        "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
        "    return rgb_image\n",
        "\n",
        "sampleImage = getImg(sampleImage)\n",
        "plt.imshow(sampleImage)"
      ],
      "id": "7WP7fsXtq_N-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7lvilmJrnG0"
      },
      "source": [
        " * train.csv 확인"
      ],
      "id": "W7lvilmJrnG0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRZQ7dr-rql9"
      },
      "source": [
        "train_csv = pd.read_csv(os.path.join(DATA_DIR, 'train', 'train.csv'))\n",
        "train_csv.sample(n=10)"
      ],
      "id": "rRZQ7dr-rql9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImDsQJ1Er4kz"
      },
      "source": [
        " * 학습 데이터의 쓰레기 분류 분포"
      ],
      "id": "ImDsQJ1Er4kz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJT3Ydl1r8hB"
      },
      "source": [
        "train_csv.groupby('category').count()"
      ],
      "id": "aJT3Ydl1r8hB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLUjlQjOsDG5"
      },
      "source": [
        " * 카테고리 딕셔너리 생성"
      ],
      "id": "cLUjlQjOsDG5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaLLTgU2sFig"
      },
      "source": [
        "category_list = train_csv['category'].unique()\n",
        "category = {}\n",
        "for i in range(len(category_list)):\n",
        "    category[category_list[i]] = i\n",
        "\n",
        "print(category)\n",
        "print(category.keys())"
      ],
      "id": "TaLLTgU2sFig",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jGsh3JuvjjZ"
      },
      "source": [
        "### 데이터 전처리 방법\n",
        "#### 해당 데이터에는 전처리가 적용된 이미지이지만 전처리 방법 소개\n",
        " * resize : 이미지 파일의 크기가 너무 커서 학습 시간이 오래 걸리거나, 각 이미지 파일의 크기가 다른 점을 보정하기 위해 지정된 규격으로 이미지 크기 통일\n",
        " * interpolation : 이미지의 비율을 변경할 때 존재하지 않는 영역에 새로운 픽셀 값을 매핑하거나, 존재하는 픽셀들을 압축하여 새로운 값을 할당. 새로운 픽셀 값은 interpolation(보간법)을 이용하여 구함.\n",
        " "
      ],
      "id": "8jGsh3JuvjjZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPeASVAdzPt9"
      },
      "source": [
        "## 3. 데이터 로드\n",
        "### 사용할 파라미터\n",
        " * BATCH_SIZE : 모델에 입력할 데이터의 단위. 전체 데이터 셋을 여러개의 소그룹으로 나누어 학습하게 되는데, batch size는 하나의 소그룹에 속하는 데이터 수가 된다"
      ],
      "id": "fPeASVAdzPt9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bSlmOvlRwse"
      },
      "source": [
        "# 데이터 로드 파라미터\n",
        "BATCH_SIZE = 128\n",
        "INPUT_SHAPE = (128, 128)\n",
        "\n",
        "# train, validation 비율\n",
        "VAL_RATIO = 0.2"
      ],
      "id": "3bSlmOvlRwse",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeLIsxeaR9bK"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_dir, mode, input_shape):\n",
        "        self.data_dir = os.path.join(data_dir, 'train')\n",
        "        self.mode = mode\n",
        "        self.input_shape = input_shape\n",
        "        self.db = self.data_loader()\n",
        "        self.transform = transforms.Compose([transforms.Resize(self.input_shape), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) \n",
        "        self.class_num = len(self.db['label'].unique())\n",
        "\n",
        "    def data_loader(self):\n",
        "        print(\"Loading \" + self.mode + \" dataset...\")\n",
        "\n",
        "        img_dir = os.path.join(self.data_dir, 'images')\n",
        "        label_dir = os.path.join(self.data_dir, 'train.csv')\n",
        "\n",
        "        #이미지 폴더 존재 여부 체크\n",
        "        if not os.path.isdir(img_dir):\n",
        "            print(f'!!! Cannot find {img_dir} !!!')\n",
        "            sys.exit()\n",
        "\n",
        "        #라벨 파일 존재 여부 체크\n",
        "        if not os.path.lexists(label_dir):\n",
        "            print(f'!!! Cannot find {label_dir} !!!')\n",
        "            sys.exit()\n",
        "\n",
        "        category_ls = list(category.keys())\n",
        "\n",
        "        # train.csv를 pandas로 불러오기\n",
        "        df = train_csv\n",
        "\n",
        "        # return할 train, val 데이터 초기화\n",
        "        db_train = db_val = pd.DataFrame(columns=['img_path','label'])\n",
        "\n",
        "        # 카테고리별로 train/val 비율 맞춰 나누기 위해 카테고리별로 루프 실행\n",
        "        for cat in category_ls:\n",
        "            img_path_list = []\n",
        "            img_label_list = []\n",
        "\n",
        "            # 특정 카테고리에 해당하는 이미지 리스트 뽑기\n",
        "            img_list = df['file_name'].loc[df['category']==cat].values\n",
        "\n",
        "            for filename in img_list:\n",
        "                if os.path.lexists(os.path.join(img_dir, filename)):\n",
        "                    img_path_list.append(os.path.join(img_dir, filename))\n",
        "                    img_label_list.append(category[cat])\n",
        "            \n",
        "            db = pd.DataFrame({'img_path': img_path_list, 'label':img_label_list})\n",
        "\n",
        "            # 학습, 검증 셋 나누기\n",
        "            train, val = train_test_split(db, test_size=VAL_RATIO, random_state=42, shuffle=True)\n",
        "\n",
        "            db_train = pd.concat([db_train, train])\n",
        "            db_val = pd.concat([db_val, val])\n",
        "\n",
        "        db_train = db_train.sample(frac=1).reset_index(drop=True)\n",
        "        db_val = db_val.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            db = db_train\n",
        "        elif self.mode == 'val':\n",
        "            db = db_train\n",
        "        else:\n",
        "            print('check your mode : ')\n",
        "        \n",
        "        print(db.head())\n",
        "        return db\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.db)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        data = copy.deepcopy(self.db.loc[index])\n",
        "\n",
        "        #1. load image\n",
        "        cvimg = cv2.imread(data['img_path'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
        "        if not isinstance(cvimg, np.ndarray):\n",
        "            raise IOError(\"Fail to read %s\" % data['img_path'])\n",
        "        \n",
        "        #2. preprocessing imges\n",
        "        trans_imge = self.transform(Imgae.fromarray(cvimg))\n",
        "        return trans_image, data['label']\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = CustomDataset(data_dir=DATA_DIR, mode='train', input_shape=INPUT_SHAPE)\n",
        "validation_dataset = CustomDataset(data_dir=DATA_DIR, mode='val', input_shape=INPUT_SHAPE)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "id": "JeLIsxeaR9bK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NOULPgHsM9W"
      },
      "source": [
        "### 모델 설계 / pretrained model 불러오기\n",
        "#### 사용할 파라미터\n",
        " * LEARNING_RATE : \n",
        "  * 경사하강법을 통해 loss function의 minimum 값을 찾아다닐 때, 그 탐색 과정에 있어서의 보폭\n",
        " * EPOCHS : \n",
        "  * 한번의 epoch는 인공 신경망에서 전체 데이터 셋에 대해 forward pass / backward pass 과정을 거친 것\n",
        "  * epoch가 1만큼 지나면, 전체 데이터 셋에 대해 한번의 학습이 완료 된것\n",
        "  * 모델을 만들 때 적절한 epoch 값을 설정 해야만 underfitting 과 overfitting을 방지\n",
        " * EARLY_STOPPING_PATIENCE : \n",
        "  * 너무 많은 epoch는 overfitting을 일으키고 너무 적은  epoch는 underfitting을 일으키므로 특정 시점에 학습을 멈추는 방법이 early stopping\n",
        "  * 해당 변수는 validation score 가 개선되지 않아도 학습을 몇 에폭 더 진행할지 결정.\n",
        " * WEIGHT_DECAY : \n",
        "  * orverfitting을 억제하는 학습 기법의 하나로, 학습된 모델의 복잡도를 줄이기 위해서 학습 중  weight가 너무 큰 값을 가지지 않도록 Loss function에 Weight가 커질 경우에 대한 패널티 항목을 넣는다"
      ],
      "id": "1NOULPgHsM9W"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCovA3XBvKz6"
      },
      "source": [
        "# hyper-parameters\n",
        "LEARNING_RATE = 0.0005\n",
        "EPOCHS = 10\n",
        "\n",
        "EARLY_STOPPING_PATIENCE = 10\n",
        "WEIGHT_DECAY = 0.00001\n",
        "\n",
        "OPTIMIZER = ''\n",
        "SCHEDULER = ''\n",
        "MOMENTUM = ''\n",
        "LOSS_FN = ''\n",
        "METRIC_FN = ''"
      ],
      "id": "aCovA3XBvKz6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpbNHiZP8_Ul"
      },
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "id": "lpbNHiZP8_Ul",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwqGvVyh6oB-"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "class TrashClassifier(nn.Module):\n",
        "    def __init__(self, num_class):\n",
        "        super(TrashClassifier, self).__init__()\n",
        "        self.model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_class)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_img):\n",
        "        x = self.model(input_img)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "# 모델 생성\n",
        "model = TrashClassifier(num_class=train_dataset.class_num).to(device)"
      ],
      "id": "HwqGvVyh6oB-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GllswWFC9SYR"
      },
      "source": [
        "### 학습에 필요한 함수 설정\n",
        " * optimizer : \n",
        "  * 손실함수 값이 최소가 되는 부분을 찾기 위해 학습율과 기울기를 다양하게 수정하여 가중치를 변경시키는 것을 최적화라 하고, 최적화의 다양한 방식들을 옵티마이저라 한다.\n",
        " * scheduler : \n",
        "  * Learning rate scheduler는 미리 학습 일정을 정해두고, 그 일정에 따라 학습률을 조정하게 한다. 다시말해 Learning rate가 어떻게 변화하게 할지 정한다\n",
        " * metric_fn : \n",
        "  * 학습에서 평가지표(metric)는 validation에서 훈련된 모델이 얼마나 잘 학습되고 있는지 확인하며, 훈련 과정을 모니터링 하는데 사용"
      ],
      "id": "GllswWFC9SYR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3kzoZ_Y-R3l"
      },
      "source": [
        "import sklearn.metrics as metrics\n",
        "\n",
        "def get_metric_fn(y_pred, y_answer, y_prod):\n",
        "    assert len(y_pred) == len(y_answer), 'The size of prediction and answer are not same.'\n",
        "    f1 = metrics.f1_score(y_answer, y_pred, average=\"macro\")\n",
        "    return f1, 1\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e5, max_lr=0.0001, epochs=EPOCHS, steps_per_epoch=len(train_dataloader))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "metric_fn = get_metric_fn"
      ],
      "id": "I3kzoZ_Y-R3l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMIB3-e-CRtO"
      },
      "source": [
        "### Trainer 설정\n",
        " * epoch 별 학습/검증 절차를 정의"
      ],
      "id": "SMIB3-e-CRtO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYZFh_D2Cj5c"
      },
      "source": [
        "import tqdm\n",
        "\n",
        "class Trainer():\n",
        "    def __init__(self, criterion, model, device, metric_fn, optimizer=None, scheduler=None, logger=None):\n",
        "        self.criterion = criterion\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.optimizer = optimizer\n",
        "        self.logger = logger\n",
        "        self.scheduler = scheduler\n",
        "        self.metric_fn = metric_fn\n",
        "\n",
        "    def train_epoch(self, dataloader, epoch_index):\n",
        "        #한 epoch에서 수행되는 학습 절차\n",
        "        self.model.train()\n",
        "        train_total_loss = 0\n",
        "        target_lst = []\n",
        "        pred_lst = []\n",
        "        prob_lst = []\n",
        "\n",
        "        for batch_index, (img, label) in enumerate(dataloader):\n",
        "            img = img.to(self.device)\n",
        "            label = label.to(self.device).long()\n",
        "            pred = self.model(img)\n",
        "            loss = self.criterion(pred, label)\n",
        "\n",
        "            ## pytorch 에서는 gradients 값들을 추후에 backward를 해줄때 계속 더해주기 때문에\n",
        "            ## 항상 backpropagation을 하기전에 gradients를 0로 만들어주고 시작\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step()\n",
        "            train_total_loss += loss.item()\n",
        "            prob_lst.extend(pred[:,1].cpu().tolist())\n",
        "            target_lst.extend(label.cpu().tolist())\n",
        "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
        "\n",
        "        self.train_mean_loss = train_total_loss / batch_index\n",
        "        self.train_score, auroc = self.metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
        "        msg = f'Epoch {epoch_index}, Train loss: {self.train_mean_loss}, Acc: {self.train_score}'\n",
        "        print(msg)\n",
        "\n",
        "    def validate_epoch(self, dataloader, epoch_index, mode=None):\n",
        "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
        "\n",
        "        Args:\n",
        "            dataloader (`dataloader`)\n",
        "            epoch_index (int)\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        val_total_loss = 0\n",
        "        target_lst = []\n",
        "        pred_lst = []\n",
        "        prob_lst = []\n",
        "        with torch.no_grad():\n",
        "            for batch_index, (img, label) in enumerate(dataloader):\n",
        "                img = img.to(self.device)\n",
        "                label = label.to(self.device).long()\n",
        "                pred = self.model(img)\n",
        "                loss = self.criterion(pred, label)\n",
        "                val_total_loss += loss.item()\n",
        "                prob_lst.extend(pred[:, 1].cpu().tolist())\n",
        "                target_lst.extend(label.cpu().tolist())\n",
        "                pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
        "            self.val_mean_loss = val_total_loss / batch_index\n",
        "            self.validation_score, auroc = self.metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
        "            msg = f'Epoch {epoch_index}, {mode} loss: {self.val_mean_loss}, Acc: {self.validation_score}, ROC: {auroc}'\n",
        "            print(msg)\n",
        "        #self.logger.info(msg) if self.logger else print(msg)\n",
        "\n",
        "        \n",
        "# Set trainer\n",
        "trainer = Trainer(criterion, model, device, metric_fn, optimizer, scheduler, logger=system_logger)\n"
      ],
      "id": "dYZFh_D2Cj5c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Earlystopper 세팅\n"
      ],
      "metadata": {
        "id": "iNFK1Fk_2Bft"
      },
      "id": "iNFK1Fk_2Bft"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "class LossEarlyStopper():\n",
        "    def __init__(self, patience:int, verbose: bool, logger:logging.RootLogger=None) -> None:\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.patience_counter = 0\n",
        "        self.min_loss = np.Inf\n",
        "        self.logger = logger\n",
        "        self.stop = False\n",
        "\n",
        "    def check_early_stopping(self, loss: float) -> None:\n",
        "        if self.min_loss == np.Inf:\n",
        "            #첫 에폭\n",
        "            self.min_loss = loss\n",
        "\n",
        "        elif loss > self.min_loss:\n",
        "            #loss가 줄지 않음 -> patience_counter 1 증가\n",
        "            self.patience_counter += 1\n",
        "            msg = f'Early stopper, Early stopping counter {self.patience_counter} / {self.patience}'\n",
        "\n",
        "            if self.patience_counter == self.patience:\n",
        "                self.stop = True\n",
        "            if self.verbose:\n",
        "                self.logger.info(msg) if self.logger else print(msg)\n",
        "        \n",
        "        elif loss <= self.min_loss:\n",
        "            #loss가 줄어듬 -> min_loss 갱신\n",
        "            self.save_model = True\n",
        "            msg = f\"Early stopper, Validation loss decreased {self.min_loss} -> {loss}\"\n",
        "            self.min_loss = loss\n",
        "\n",
        "            if self.verbose:\n",
        "                self.logger.info(msg) if self.logger else print(msg)\n",
        "\n",
        "early_stopper = LossEarlyStopper(patience=EARLY_STOPPING_PATIENCE, verbose=True, logger=system_logger)"
      ],
      "metadata": {
        "id": "oUj71uk82Rdc"
      },
      "id": "oUj71uk82Rdc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perfomance Recorder 셋팅"
      ],
      "metadata": {
        "id": "mqap4sEx33Pw"
      },
      "id": "mqap4sEx33Pw"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import logging\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class PerformanceRecorder():\n",
        "\n",
        "    def __init__(self,\n",
        "                 column_name_list: list,\n",
        "                 record_dir: str,\n",
        "                 key_column_value_list: list,\n",
        "                 model: 'model',\n",
        "                 optimizer: 'optimizer',\n",
        "                 scheduler: 'scheduler',\n",
        "                 logger: logging.RootLogger=None):\n",
        "        \"\"\"Recorder 초기화\n",
        "            \n",
        "        Args:\n",
        "            column_name_list (list(str)):\n",
        "            record_dir (str):\n",
        "            key_column_value_list (list)\n",
        "\n",
        "        Note:\n",
        "        \"\"\"\n",
        "\n",
        "        self.column_name_list = column_name_list\n",
        "        \n",
        "        self.record_dir = record_dir\n",
        "        self.record_filepath = os.path.join(self.record_dir, 'record.csv')\n",
        "        self.best_record_filepath = os.path.join('/'.join(self.record_dir.split('/')[:-1]),'train_best_record.csv')\n",
        "        self.weight_path = os.path.join(record_dir, 'model.pt')\n",
        "\n",
        "        self.row_counter = 0\n",
        "        self.key_column_value_list = key_column_value_list\n",
        "\n",
        "        self.train_loss_list = list()\n",
        "        self.validation_loss_list= list()\n",
        "        self.train_score_list = list()\n",
        "        self.validation_score_list = list()\n",
        "\n",
        "        self.loss_plot = None\n",
        "        self.score_plot = None\n",
        "\n",
        "        self.min_loss = np.Inf\n",
        "        self.best_record = None\n",
        "\n",
        "        self.logger = logger\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "        self.key_column_value_list = key_column_value_list\n",
        "\n",
        "    def set_model(self, model: 'model'):\n",
        "        self.model = model\n",
        "\n",
        "    def set_logger(self, logger: logging.RootLogger):\n",
        "        self.logger = logger\n",
        "\n",
        "    def create_record_directory(self):\n",
        "        \"\"\"\n",
        "        record 경로 생성\n",
        "        \"\"\"\n",
        "        make_directory(self.record_dir)\n",
        "        msg = f\"Create directory {self.record_dir}\"\n",
        "        self.logger.info(msg) if self.logger else None\n",
        "\n",
        "    def add_row(self,\n",
        "                epoch_index: int,\n",
        "                train_loss: float,\n",
        "                validation_loss: float,\n",
        "                train_score: float,\n",
        "                validation_score: float):\n",
        "        \"\"\"Epoch 단위 성능 적재\n",
        "        \n",
        "        최고 성능 Epoch 모니터링\n",
        "        모든 Epoch 종료 이후 최고 성능은 train_best_records.csv 에 적재\n",
        "\n",
        "        Args:\n",
        "            row (list): \n",
        "\n",
        "        \"\"\"\n",
        "        self.train_loss_list.append(train_loss)\n",
        "        self.validation_loss_list.append(validation_loss)\n",
        "        self.train_score_list.append(train_score)\n",
        "        self.validation_score_list.append(validation_score)\n",
        "\n",
        "        row = self.key_column_value_list + [epoch_index, train_loss, validation_loss, train_score, validation_score]\n",
        "        \n",
        "        # Update scheduler learning rate\n",
        "        learning_rate = self.scheduler.get_last_lr()[0]\n",
        "        row[9] = learning_rate\n",
        "\n",
        "        with open(self.record_filepath, newline='', mode='a') as f:\n",
        "            writer = csv.writer(f)\n",
        "\n",
        "            if self.row_counter == 0:\n",
        "                writer.writerow(self.column_name_list)\n",
        "\n",
        "            writer.writerow(row)\n",
        "            msg = f\"Write row {self.row_counter}\"\n",
        "            self.logger.info(msg) if self.logger else None\n",
        "\n",
        "        self.row_counter += 1\n",
        "\n",
        "        # Update best record & Save check point\n",
        "        if validation_loss < self.min_loss:\n",
        "            msg = f\"Update best record row {self.row_counter}, checkpoints {self.min_loss} -> {validation_loss}\"\n",
        "            \n",
        "            self.min_loss = validation_loss\n",
        "            self.best_record = row\n",
        "            self.save_weight()\n",
        "\n",
        "            self.logger.info(msg) if self.logger else None\n",
        "\n",
        "    def add_best_row(self):\n",
        "        \"\"\"\n",
        "        모든 Epoch 종료 이후 최고 성능에 해당하는 row을 train_best_records.csv 에 적재\n",
        "        \"\"\"\n",
        "\n",
        "        n_row = count_csv_row(self.best_record_filepath)\n",
        "\n",
        "        with open(self.best_record_filepath, newline='', mode='a') as f:\n",
        "            writer = csv.writer(f)\n",
        "\n",
        "            if n_row == 0:\n",
        "               writer.writerow(self.column_name_list)\n",
        "            \n",
        "            writer.writerow(self.best_record)\n",
        "\n",
        "        msg = f\"Save best record {self.best_record_filepath}\"\n",
        "        self.logger.info(msg) if self.logger else None\n",
        "\n",
        "    def save_weight(self)-> None:\n",
        "        \"\"\"Weight 저장\n",
        "\n",
        "        Args:\n",
        "            loss (float): validation loss\n",
        "            model (`model`): model\n",
        "        \n",
        "        \"\"\"\n",
        "        check_point = {\n",
        "            'model': self.model.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'scheduler': self.scheduler.state_dict()\n",
        "        }\n",
        "        torch.save(check_point, self.weight_path)\n",
        "        #torch.save(self.model.state_dict(), self.weight_path)\n",
        "        msg = f\"Model saved: {self.weight_path}\"\n",
        "        self.logger.info(msg) if self.logger else None\n",
        "\n",
        "    def save_performance_plot(self, final_epoch: int):\n",
        "        \"\"\"Epoch 단위 loss, score plot 생성 후 저장\n",
        "\n",
        "        \"\"\"\n",
        "        self.loss_plot = self.plot_performance(epoch=final_epoch+1,\n",
        "                                          train_history=self.train_loss_list,\n",
        "                                          validation_history=self.validation_loss_list,\n",
        "                                          target='loss')\n",
        "        self.score_plot = self.plot_performance(epoch=final_epoch+1,\n",
        "                                           train_history=self.train_score_list,\n",
        "                                           validation_history=self.validation_score_list,\n",
        "                                           target='score')\n",
        "\n",
        "        self.loss_plot.savefig(os.path.join(self.record_dir, 'loss.png'))\n",
        "        self.score_plot.savefig(os.path.join(self.record_dir, 'score.jpg'))\n",
        "        plt.close('all')\n",
        "\n",
        "\n",
        "        msg = f\"Save performance plot {self.record_dir}\"\n",
        "        self.logger.info(msg) if self.logger else None\n",
        "\n",
        "    def plot_performance(self,\n",
        "                         epoch: int,\n",
        "                         train_history:list,\n",
        "                         validation_history:list,\n",
        "                         target:str)-> plt.figure:\n",
        "        \"\"\"loss, score plot 생성\n",
        "\n",
        "        \"\"\"\n",
        "        fig = plt.figure(figsize=(12, 5))\n",
        "        epoch_range = list(range(epoch))\n",
        "\n",
        "        plt.plot(epoch_range, train_history, marker='.', c='red', label=\"train\")\n",
        "        plt.plot(epoch_range, validation_history, marker='.', c='blue', label=\"validation\")\n",
        "\n",
        "        plt.legend(loc='upper right')\n",
        "        plt.grid()\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel(target)\n",
        "\n",
        "        return fig"
      ],
      "metadata": {
        "id": "0UD7Aqq6396z"
      },
      "id": "0UD7Aqq6396z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key_column_value_list = [\n",
        "    TRAIN_SERIAL,\n",
        "    TRAIN_TIMESTAMP,\n",
        "    MODEL,\n",
        "    OPTIMIZER,\n",
        "    LOSS_FN,\n",
        "    METRIC_FN,\n",
        "    EARLY_STOPPING_PATIENCE,\n",
        "    BATCH_SIZE,\n",
        "    EPOCHS,\n",
        "    LEARNING_RATE,\n",
        "    WEIGHT_DECAY,\n",
        "    RANDOM_SEED]\n",
        "\n",
        "performance_recorder = PerformanceRecorder(column_name_list=PERFORMANCE_RECORD_COLUMN_NAME_LIST,\n",
        "                                           record_dir=PERFORMANCE_RECORD_DIR,\n",
        "                                           key_column_value_list=key_column_value_list,\n",
        "                                           logger=system_logger,\n",
        "                                           model=model,\n",
        "                                           optimizer=optimizer,\n",
        "                                           scheduler=scheduler)\n"
      ],
      "metadata": {
        "id": "PA4WKR5J4Jg1"
      },
      "id": "PA4WKR5J4Jg1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습\n",
        " * 각 에폭별 수행할 절차를 trainer 클래스의 train_epoch, validation_epoch을 이용해 정의\n",
        " * EPOCHS만큼, 혹은 early_stop이 될 때까지 에폭을 반"
      ],
      "metadata": {
        "id": "fc31yUkF4Lki"
      },
      "id": "fc31yUkF4Lki"
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "criterion = 1E+8\n",
        "\n",
        "# 저장될 모델 이름\n",
        "model_name = 'best.pt'\n",
        "\n",
        "# 에폭 별로 train_epoch, valid_epoch 실헹힘\n",
        "for epoch_index in tqdm(range(EPOCHS)):\n",
        "\n",
        "    trainer.train_epoch(train_dataloader, epoch_index)\n",
        "    trainer.validate_epoch(validation_dataloader, epoch_index, 'val')\n",
        "\n",
        "    # Performance record - csv & save elapsed_time\n",
        "    performance_recorder.add_row(epoch_index=epoch_index,\n",
        "                                 train_loss=trainer.train_mean_loss,\n",
        "                                 validation_loss=trainer.val_mean_loss,\n",
        "                                 train_score=trainer.train_score,\n",
        "                                 validation_score=trainer.validation_score)\n",
        "\n",
        "    # Performance record - plot\n",
        "    performance_recorder.save_performance_plot(final_epoch=epoch_index)\n",
        "\n",
        "    # early_stopping check\n",
        "    early_stopper.check_early_stopping(loss=trainer.val_mean_loss)\n",
        "\n",
        "    if early_stopper.stop:\n",
        "        print('Early stopped')\n",
        "        break\n",
        "    \n",
        "    if trainer.val_mean_loss < criterion:\n",
        "        criterion = trainer.val_mean_loss\n",
        "        performance_recorder.weight_path = os.path.join(PERFORMANCE_RECORD_DIR, model_name)\n",
        "        performance_recorder.save_weight()\n",
        "        # print(f'{epoch_index} model saved train acc :{trainer.train_score}, val acc :{trainer.validation_score}')\n",
        "        print(f'{epoch_index} model saved')\n",
        "        print('----------------------------------')\n",
        "\n",
        "\n",
        "print(\"Model saved: \", os.path.join(PERFORMANCE_RECORD_DIR, model_name))\n",
        "print(\"Train score saved: \",os.path.join(PERFORMANCE_RECORD_DIR, 'score.jpg'))\n",
        "print(\"Train loss saved: \",os.path.join(PERFORMANCE_RECORD_DIR, 'loss.png'))"
      ],
      "metadata": {
        "id": "dwY8W73T4beN"
      },
      "id": "dwY8W73T4beN",
      "execution_count": null,
      "outputs": []
    }
  ]
}